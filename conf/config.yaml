# Federated Instruction Tuning on General Dataset
---

num_clients: 2 # total number of clients
num_rounds: 1

dataset:
  name: "/content/drive/MyDrive/client_1_data"

model:
  name: "/content/drive/MyDrive/llava-v1.5-7b-quilt-merged_new"
  quantization: 4 # 8 or 4 if you want to do quantization with BitsAndBytes
  gradient_checkpointing: True
  lora:
    peft_lora_r: 32
    peft_lora_alpha: 64

train:
  num_rounds: ${num_rounds}
  save_every_round: 1
  learning_rate_max: 5e-5
  learning_rate_min: 1e-6
  seq_length: 512
  training_arguments :
    output_dir: null
    # report_to: "tensorboard"
    learning_rate: 1.4e-5
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 4
    logging_steps: 10
    num_train_epochs: 1
    max_steps: 10
    save_steps: 1000
    save_total_limit: 10
    report_to: null
    # push_to_hub=True,
    gradient_checkpointing: ${model.gradient_checkpointing}
    lr_scheduler_type: "constant"
    remove_unused_columns: False
    fp16: True
    bf16: False
  # training_arguments:
  #   output_dir: null # to be set by hydra
  #   learning_rate: null # to be set by the client
  #   per_device_train_batch_size: 4
  #   gradient_accumulation_steps: 4
  #   logging_steps: 10
  #   num_train_epochs: 3
  #   max_steps: 10
  #   report_to: null
  #   save_steps: 1000
  #   save_total_limit: 10
  #   gradient_checkpointing: ${model.gradient_checkpointing}
  #   lr_scheduler_type: "constant"

strategy:
  _target_: flwr.server.strategy.FedAvg
  fraction_fit: 0.1 # sample 10% of clients (i.e. 2 per round)
  fraction_evaluate: 0.5 # no client evaluation

client_resources:
  num_cpus: 2
  num_gpus: 1.0